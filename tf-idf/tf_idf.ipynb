{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document-Term Incidence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import math\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# TF-IDF 계산식\n",
    "def tf_idf(tf, N, df):\n",
    "    return (math.log10(1 + tf))*(math.log10(N/df))\n",
    "\n",
    "# Cosine Similarity 계산식\n",
    "def cosine_similarity(x, y):\n",
    "    normalizing_factor_x = np.sqrt(np.sum(np.square(x)))\n",
    "    normalizing_factor_y = np.sqrt(np.sum(np.square(y)))\n",
    "    return np.matmul(x, np.transpose(y)/(normalizing_factor_x*normalizing_factor_y))\n",
    "\n",
    "# 리스트 공통 원소 추출 함수\n",
    "def common_list(x_list, y_list):\n",
    "    return list(set(x_list) & set(y_list))\n",
    "\n",
    "\n",
    "# 객체화\n",
    "class DocumentTermMatrix:\n",
    "    def __init__(self):\n",
    "        self.stopword_list = set(stopwords.words('english'))\n",
    "        self.snowball_stemmer = SnowballStemmer(\"english\")\n",
    "        \n",
    "        # document path\n",
    "        self.path = os.path.join('./', 'data')\n",
    "        self.file_list = sorted(os.listdir(path=self.path), key=lambda x: int(x.split('.')[0]))\n",
    "        \n",
    "        # query path\n",
    "        self.query_path = os.path.join('./', 'query')\n",
    "        self.query_file = os.listdir(path=self.query_path)\n",
    "        \n",
    "        # save document-term matrix with tf-idf\n",
    "        self.tf_idf_matrix = []\n",
    "        \n",
    "    # additional stopwords\n",
    "    def add_stopwords(self, additional_words):\n",
    "        self.stopword_list.update(additional_words)\n",
    "    \n",
    "    # document path update\n",
    "    def update_path(self, new_path):\n",
    "        self.path = new_path\n",
    "        self.file_list = sorted(os.listdir(path=self.path),key=lambda x: int(x.split('.')[0]))\n",
    "        \n",
    "    # query path update\n",
    "    def update_query_path(self, new_path):\n",
    "        self.query_path = new_path\n",
    "        self.query_file = os.listdir(path=self.query_path)\n",
    "    \n",
    "    # update language for stemmer\n",
    "    def update_stemmer_language(self, language):\n",
    "        self.stopword_list = set(stopwords.words(language))\n",
    "        self.snowball_stemmer = SnowballStemmer(language)\n",
    "\n",
    "        \n",
    "    # index\n",
    "    def all_terms(self):\n",
    "\n",
    "        # all terms in docs\n",
    "        term_list = []\n",
    "        \n",
    "        # 경로 내의 모든 파일에서 사용된 모든 토큰 리스트 생성\n",
    "        for index in range(0,len(self.file_list)):\n",
    "            with open(f\"{self.path}/{self.file_list[index]}\", 'r') as f:\n",
    "                data = f.read()\n",
    "            # tokenizing + filter stopwords + stemming\n",
    "            tokens = [self.snowball_stemmer.stem(token) for token in nltk.word_tokenize(data)\n",
    "                      if token not in self.stopword_list]\n",
    "            text = nltk.Text(tokens)\n",
    "            term_list += list(text.vocab().keys())\n",
    "            \n",
    "        # all terms in docs without duplicate\n",
    "        terms = sorted(list(set(term_list)))\n",
    "        return terms\n",
    "\n",
    "    # index\n",
    "    def index_dictionary(self):\n",
    "        \n",
    "        # Index Dictionary\n",
    "        index_dictionary = {}\n",
    "        \n",
    "        # 경로 내의 모든 파일에서 사용된 모든 토큰 리스트 생성\n",
    "        for index in range(0,len(self.file_list)):\n",
    "            with open(f\"{self.path}/{self.file_list[index]}\", 'r') as f:\n",
    "                data = f.read()\n",
    "            # tokenizing + filter stopwords + stemming\n",
    "            tokens = [self.snowball_stemmer.stem(token) for token in nltk.word_tokenize(data)\n",
    "                      if token not in self.stopword_list]\n",
    "            text = nltk.Text(tokens)\n",
    "            index_dictionary[self.file_list[index]] = text.vocab()\n",
    "        \n",
    "        return index_dictionary\n",
    "    \n",
    "    # inverted index\n",
    "    def inverted_dictionary(self):\n",
    "        \n",
    "        # Inverted Index Dictionary\n",
    "        inverted_dictionary = {}\n",
    "        all_terms = self.all_terms()\n",
    "        index_dictionary = self.index_dictionary()\n",
    "        \n",
    "        # inverted index by term created\n",
    "        for term in all_terms:\n",
    "            for doc, freq in index_dictionary.items():\n",
    "                if term in freq:\n",
    "                    if term not in inverted_dictionary:\n",
    "                        inverted_dictionary[term] = [doc]\n",
    "                    else:\n",
    "                        inverted_dictionary[term] += [doc]\n",
    "\n",
    "        return inverted_dictionary\n",
    "  \n",
    "    # Document-Term Incidence Matrix by TF-IDF\n",
    "    def create_matrix(self):\n",
    "        \n",
    "        all_terms = self.all_terms()\n",
    "        inverted_dictionary = self.inverted_dictionary()\n",
    "\n",
    "        # 경로 내의 모든 파일-모든 토큰에 대한 Document-Term Incidence Matrix\n",
    "        for index in range(0,len(self.file_list)):\n",
    "            with open(f\"{self.path}/{self.file_list[index]}\", 'r') as f:\n",
    "                data = f.read()\n",
    "            tokens = [self.snowball_stemmer.stem(token) for token in nltk.word_tokenize(data)\n",
    "                      if token not in self.stopword_list]\n",
    "            text = nltk.Text(tokens)\n",
    "\n",
    "            # 4900여개(모든 토큰 수)의 차원을 가진 영행렬\n",
    "            token_matrix = np.zeros(len(all_terms))\n",
    "\n",
    "            if index == 0:\n",
    "                for index, term in enumerate(all_terms):\n",
    "                    if term in text.vocab():\n",
    "                        # TF-IDF로 계산한 Weight를 Matrix로 채우기\n",
    "                        token_matrix[index] = tf_idf(text.vocab()[term], len(self.file_list), len(inverted_dictionary[term]))\n",
    "                doc_term_matrix = token_matrix\n",
    "\n",
    "            else:\n",
    "                for index, term in enumerate(all_terms):\n",
    "                    if term in text.vocab():\n",
    "                        token_matrix[index] = tf_idf(text.vocab()[term], len(self.file_list), len(inverted_dictionary[term]))\n",
    "                doc_term_matrix = np.vstack([doc_term_matrix, token_matrix])\n",
    "        \n",
    "        # once we build a matrix and use it without additional calculation\n",
    "        self.tf_idf_matrix = doc_term_matrix\n",
    "        return doc_term_matrix\n",
    "    \n",
    "    # get matrix we already built before\n",
    "    def get_doc_term_matrix(self):\n",
    "        if self.tf_idf_matrix == []:\n",
    "            self.tf_idf_matrix = self.create_matrix()\n",
    "        return self.tf_idf_matrix\n",
    "    \n",
    "    # Query Doc-Term Incidence Matrix by TF-IDF\n",
    "    def create_query_matrix(self, input_data):\n",
    "        \n",
    "        all_terms = self.all_terms()\n",
    "        inverted_dictionary = self.inverted_dictionary()\n",
    "\n",
    "        # 경로 내의 query파일 토큰에 대한 Document-Term Incidence Matrix\n",
    "#         with open(f\"{self.query_path}/{self.query_file[0]}\", 'r') as f:\n",
    "#             data = f.read()\n",
    "        data = input_data\n",
    "        tokens = [self.snowball_stemmer.stem(token) for token in nltk.word_tokenize(data)\n",
    "                  if token not in self.stopword_list]\n",
    "        text = nltk.Text(tokens)\n",
    "\n",
    "        # 4900여개(모든 토큰 수)의 차원을 가진 영행렬\n",
    "        token_matrix = np.zeros(len(all_terms))\n",
    "\n",
    "        for index, term in enumerate(all_terms):\n",
    "            if term in text.vocab():\n",
    "                # TF-IDF로 계산한 Weight를 Matrix로 채우기\n",
    "                token_matrix[index] = tf_idf(text.vocab()[term], len(self.file_list), len(inverted_dictionary[term]))\n",
    "        doc_term_matrix = token_matrix\n",
    "\n",
    "        return doc_term_matrix\n",
    "    \n",
    "    # Rank with Cosine Similarity between query&docs\n",
    "    def rank_documentations(self, input_data):\n",
    "        \n",
    "        inverted_dictionary = self.inverted_dictionary()\n",
    "        documentation_matrix = self.get_doc_term_matrix()\n",
    "        query_matrix = self.create_query_matrix(input_data)\n",
    "        \n",
    "        rank = {}\n",
    "        \n",
    "        # 경로 내의 query파일 읽어들여 검색 키워드 확인하고 tokenize\n",
    "#         with open(f\"{self.query_path}/{self.query_file[0]}\", 'r') as f:\n",
    "#             data = f.read()\n",
    "        data = input_data\n",
    "        k_tokens = [self.snowball_stemmer.stem(token) for token in nltk.word_tokenize(data)\n",
    "                  if token not in self.stopword_list]\n",
    "        k_text = nltk.Text(k_tokens)\n",
    "\n",
    "        # N개의 키워드가 주어질 때, 해당 키워드들을 모두 포함하고 있는 문서 찾기\n",
    "        commons = []\n",
    "\n",
    "        for i in range(len(k_tokens)):\n",
    "            if i == 0:\n",
    "                commons = inverted_dictionary[k_tokens[i]]\n",
    "            elif i > 0:\n",
    "                commons = common_list(commons, inverted_dictionary[k_tokens[i]])\n",
    "        \n",
    "        common_indexes = sorted([int(doc.split('.')[0]) + 1 for doc in commons], key=int)\n",
    "        for index in common_indexes:\n",
    "            rank[index-1] = cosine_similarity(query_matrix, documentation_matrix[index])\n",
    "        \n",
    "        return dict(sorted(rank.items(), key=operator.itemgetter(1), reverse=True)[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_keyword():\n",
    "    input_data = str(input())\n",
    "    search = DocumentTermMatrix()\n",
    "    additional_stopwords = ('.', ',', '\\'s', '!', ':', '[', ']', '?', ';', '``', 'a', 'an',  '#', '--', \"\\''\", \"\\'\", '(', ')')\n",
    "    search.add_stopwords(additional_stopwords)\n",
    "    print(search.rank_documentations(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{53: 0.1385376590109018,\n",
       " 57: 0.08265168989438201,\n",
       " 47: 0.07363465883116009,\n",
       " 49: 0.06629725101967235,\n",
       " 46: 0.0649420890340909}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = DocumentTermMatrix()\n",
    "additional_stopwords = ('.', ',', '\\'s', '!', ':', '[', ']', '?', ';', '``', 'a', 'an',  '#', '--', \"\\''\", \"\\'\", '(', ')')\n",
    "search.add_stopwords(additional_stopwords)\n",
    "search.rank_documentations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_query_matrix() missing 1 required positional argument: 'input_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-95fcc79566f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_query_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: create_query_matrix() missing 1 required positional argument: 'input_data'"
     ]
    }
   ],
   "source": [
    "search.create_query_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "x = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "president obama\n",
      "{53: 0.1385376590109018, 57: 0.08265168989438201, 47: 0.07363465883116009, 49: 0.06629725101967235, 46: 0.0649420890340909}\n"
     ]
    }
   ],
   "source": [
    "search_keyword()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trump trump obama yeah\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.create_query_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "introduction of web\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b7c7b5370329>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank_documentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-692de1d818fa>\u001b[0m in \u001b[0;36mrank_documentations\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0minverted_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommon_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverted_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverted_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mcommons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minverted_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mcommon_indexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcommons\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "search.rank_documentations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drinking with the coffee and\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'coffe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3f69faafd837>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0minverted_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommon_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverted_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverted_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mcommons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minverted_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'coffe'"
     ]
    }
   ],
   "source": [
    "search = DocumentTermMatrix()\n",
    "additional_stopwords = ('.', ',', '\\'s', '!', ':', '[', ']', '?', ';', '``', 'a', 'an',  '#', '--', \"\\''\", \"\\'\", '(', ')')\n",
    "search.add_stopwords(additional_stopwords)\n",
    "\n",
    "stopword_list = search.stopword_list\n",
    "snowball_stemmer = search.snowball_stemmer\n",
    "inverted_dictionary = search.inverted_dictionary()\n",
    "\n",
    "data = str(input())\n",
    "k_tokens = [snowball_stemmer.stem(token) for token in nltk.word_tokenize(data)\n",
    "          if token not in stopword_list]\n",
    "k_text = nltk.Text(k_tokens)\n",
    "\n",
    "# N개의 키워드가 주어질 때, 해당 키워드들을 모두 포함하고 있는 문서 찾기\n",
    "if len(k_tokens) == 1:\n",
    "    commons = inverted_dictionary[k_tokens[0]]\n",
    "\n",
    "else:\n",
    "    i = 0\n",
    "    while i in range(len(k_tokens)-1):\n",
    "        inverted_dictionary[k_tokens[i+1]] = common_list(inverted_dictionary[k_tokens[i]], inverted_dictionary[k_tokens[i+1]])\n",
    "        i += 1\n",
    "    commons = inverted_dictionary[k_tokens[-1]]\n",
    "    \n",
    "commons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "president president obama obama\n",
      "['6.txt', '54.txt', '49.txt', '50.txt', '36.txt', '48.txt', '53.txt', '47.txt', '46.txt', '58.txt', '41.txt', '57.txt', '40.txt']\n"
     ]
    }
   ],
   "source": [
    "search = DocumentTermMatrix()\n",
    "additional_stopwords = ('.', ',', '\\'s', '!', ':', '[', ']', '?', ';', '``', 'a', 'an',  '#', '--', \"\\''\", \"\\'\", '(', ')')\n",
    "search.add_stopwords(additional_stopwords)\n",
    "\n",
    "stopword_list = search.stopword_list\n",
    "snowball_stemmer = search.snowball_stemmer\n",
    "inverted_dictionary = search.inverted_dictionary()\n",
    "\n",
    "data = str(input())\n",
    "k_tokens = [snowball_stemmer.stem(token) for token in nltk.word_tokenize(data)\n",
    "          if token not in stopword_list]\n",
    "\n",
    "commons = []\n",
    "\n",
    "for i in range(len(k_tokens)):\n",
    "    if i == 0:\n",
    "        commons = inverted_dictionary[k_tokens[i]]\n",
    "    elif i > 0:\n",
    "        commons = common_list(commons, inverted_dictionary[k_tokens[i]])\n",
    "\n",
    "print(commons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info_retrieval",
   "language": "python",
   "name": "info_retrieval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
